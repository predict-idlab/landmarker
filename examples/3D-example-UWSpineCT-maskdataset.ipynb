{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <a href=\"https://predict-idlab.github.io/landmarker\">\n",
    "        <img alt=\"landmarker\" src=\"https://raw.githubusercontent.com/predict-idlab/landmarker/main/docs/_static/images/logo.svg\" width=\"66%\">\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "# Training and Evaluating One-hot Encoded (Mask) Regression Model for Landmark Localizatioin on UWSpineCT (3D)\n",
    "\n",
    "In this tutorial, we will train and evaluate an one-hot encoded (mask) regression model for landmark \n",
    "localization on UWSpineCT. The UWSpineCT dataset, which consists of XX\n",
    "annotated CT volumes of the spine. The CT volumes are transformed to a uniform scale of 128 × 128 × 64.\n",
    "\n",
    "We will go through the following steps:\n",
    "* [Loading the dataset](#Loading-the-dataset)\n",
    "* [Inspecting the dataset](#Inspecting-the-dataset)\n",
    "* [Training and initializing the UNet model](#Training-the-model)\n",
    "* [Evaluating the model](#Evaluating-the-model)\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/predict-idlab/landmarker/examples/3D-example-UWSpineCT-maskdataset.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -c \"import landmarker\" || pip install landmarker\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"../src/\")\n",
    "import landmarker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "### Short description of the data and dataset module\n",
    "The [landmarker](https://github.com/predict-idlab/landmarker) package has several built-in\n",
    "datasets in the `landmarker.datasets` module, as well as utility classes for building your own\n",
    "datasets in the `landmarker.data` module. There are three types of datasets: 'LandmarkDataset',\n",
    "'HeatmapDataset', and 'MaskDataset'. The 'LandmarkDataset' is a dataset of images with landmarks,\n",
    "the 'HeatmapDataset' is a dataset of images with heatmaps, and the 'MaskDataset' is a dataset of\n",
    "images with masks (i.e., binary segmentation masks indiciating the location of the landmarks). The \n",
    "'HeatmapDataset' and 'MaskDataset' both inherit from the 'LandmarkDataset' class, and thus also \n",
    "contain information about the landmarks. The 'MaskDataset' can be constructed from specified image \n",
    "and landmarks pairs, or from images and masks pairs, because often that is how the data is\n",
    "distributed. The 'HeatmapDataset' can be constructed from images and landmarks pairs.\n",
    "\n",
    "Images can be provided as a list of paths to stored images, or as a a numpy arary, torch tensor, \n",
    "list of numpy  arrays or list of torch tensors. Landmarks can be as numpy arrays or torch tensors.\n",
    "These landmarks can be provided in three different shapes: (1) (N, D) where N is the number of\n",
    "samples and D is the number of dimensions, (2) (N, C, D) where C is the number of landmark\n",
    "classes, (3) (N, C, I, D) where I is the number of instances per landmark class, if less than I\n",
    "instances are provided, the remaining instances are filled with NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import (Compose, ScaleIntensityd)\n",
    "\n",
    "fn_keys = ('image', 'mask')\n",
    "spatial_transformd = []\n",
    "\n",
    "train_transformd = Compose([\n",
    "                            ScaleIntensityd(('image', )),  # Scale intensity\n",
    "                        ] + spatial_transformd)\n",
    "\n",
    "inference_transformd = Compose([\n",
    "    ScaleIntensityd(('image', )),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "path_data = \"/Users/jefjonkers/Data/landmark-datasets/UWSpineCT\"\n",
    "\n",
    "# Warning ensure that lists of paths for volumes, landmarks, and pixel_spacings\n",
    "\n",
    "# volume paths\n",
    "volume_paths_train_1 = sorted(glob(f\"{path_data}/spine-1/*/*/*.nii.gz\"))\n",
    "volume_paths_train_2 = sorted(glob(f\"{path_data}/spine-2/*/*/*.nii.gz\"))\n",
    "volume_paths_train_3 = sorted(glob(f\"{path_data}/spine-3/*/*/*.nii.gz\"))\n",
    "volume_paths_train_4 = sorted(glob(f\"{path_data}/spine-4/*/*/*.nii.gz\"))\n",
    "volume_paths_train_5 = sorted(glob(f\"{path_data}/spine-5/*/*/*.nii.gz\"))\n",
    "volume_paths_test = sorted(glob(f\"{path_data}/spine-test-data/*.nii.gz\"))\n",
    "volume_paths_train = volume_paths_train_1 + volume_paths_train_2 + volume_paths_train_3 + volume_paths_train_4\n",
    "volume_paths_val = volume_paths_train_5\n",
    "\n",
    "# landmark paths and transform to single numpy arrays for each set\n",
    "landmark_paths_train_1 = sorted(glob(f\"{path_data}/spine-1/*/*/*.lml\"))\n",
    "landmark_paths_train_2 = sorted(glob(f\"{path_data}/spine-2/*/*/*.lml\"))\n",
    "landmark_paths_train_3 = sorted(glob(f\"{path_data}/spine-3/*/*/*.lml\"))\n",
    "landmark_paths_train_4 = sorted(glob(f\"{path_data}/spine-4/*/*/*.lml\"))\n",
    "landmark_paths_train_5 = sorted(glob(f\"{path_data}/spine-5/*/*/*.lml\"))\n",
    "landmark_paths_test = sorted(glob(f\"{path_data}/spine-test-data/*.lml\"))\n",
    "landmark_paths_train = landmark_paths_train_1 + landmark_paths_train_2 + landmark_paths_train_3 + landmark_paths_train_4\n",
    "landmark_paths_val = landmark_paths_train_5\n",
    "\n",
    "df_landmarks_train = []\n",
    "for i, path in enumerate(landmark_paths_train):\n",
    "    df = pd.read_csv(path,\n",
    "                     sep='\\s+',\n",
    "                     skiprows=1,\n",
    "                     header=None)\n",
    "    df.columns = [\"ID\", \"Label\", \"Y\", \"X\", \"Z\"]\n",
    "    df[\"file_id\"] = f\"train_{i:03d}\"\n",
    "    df[\"source_file\"] = path   # keep track of origin file\n",
    "    df_landmarks_train.append(df)\n",
    "\n",
    "df_landmarks_train = pd.concat(df_landmarks_train)\n",
    "\n",
    "df_landmarks_val = []\n",
    "for i, path in enumerate(landmark_paths_val):\n",
    "    df = pd.read_csv(path,\n",
    "                     sep='\\s+',\n",
    "                     skiprows=1,\n",
    "                     header=None)\n",
    "    df.columns = [\"ID\", \"Label\", \"Y\", \"X\", \"Z\"]\n",
    "    df[\"file_id\"] = f\"val_{i:03d}\"\n",
    "    df[\"source_file\"] = path   # keep track of origin file\n",
    "    df_landmarks_val.append(df)\n",
    "\n",
    "df_landmarks_val = pd.concat(df_landmarks_val)\n",
    "\n",
    "df_landmarks_test = []\n",
    "for i, path in enumerate(landmark_paths_test):\n",
    "    df = pd.read_csv(path,\n",
    "                     sep='\\s+',\n",
    "                     skiprows=1,\n",
    "                     header=None)\n",
    "    df.columns = [\"ID\", \"Label\", \"Y\", \"X\", \"Z\"]\n",
    "    df[\"file_id\"] = f\"test_{i:03d}\"\n",
    "    df[\"source_file\"] = path   # keep track of origin file\n",
    "    df_landmarks_test.append(df)\n",
    "\n",
    "df_landmarks_test = pd.concat(df_landmarks_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "class_names = df_landmarks_train[\"Label\"].unique().tolist()\n",
    "\n",
    "landmarks_train = np.zeros((len(df_landmarks_train[\"file_id\"].unique()), len(class_names), 3), dtype=np.float32) * np.nan\n",
    "spacing_train = np.zeros((len(df_landmarks_train[\"file_id\"].unique()), 3), dtype=np.float32)\n",
    "for i, file_id in enumerate(df_landmarks_train[\"file_id\"].unique()):\n",
    "    img = nib.load(volume_paths_train[i])\n",
    "    spacing_train[i] = img.header.get_zooms()[:3]\n",
    "    affine = img.affine\n",
    "    affine_inv = np.linalg.inv(affine)\n",
    "    df = df_landmarks_train[df_landmarks_train[\"file_id\"] == file_id]\n",
    "    for j, class_name in enumerate(class_names):\n",
    "        coords = df[df[\"Label\"] == class_name][[\"Y\", \"X\", \"Z\"]].values\n",
    "        if len(coords) > 0:\n",
    "            coords = coords[0]\n",
    "            landmarks_train[i, j] = coords / spacing_train[i]\n",
    "\n",
    "landmarks_val = np.zeros((len(df_landmarks_val[\"file_id\"].unique()), len(class_names), 3), dtype=np.float32) * np.nan\n",
    "spacing_val = np.zeros((len(df_landmarks_val[\"file_id\"].unique()), 3), dtype=np.float32)\n",
    "for i, file_id in enumerate(df_landmarks_val[\"file_id\"].unique()):\n",
    "    img = nib.load(volume_paths_val[i])\n",
    "    spacing_val[i] = img.header.get_zooms()[:3]\n",
    "    affine = img.affine\n",
    "    affine_inv = np.linalg.inv(affine)\n",
    "    df = df_landmarks_val[df_landmarks_val[\"file_id\"] == file_id]\n",
    "    for j, class_name in enumerate(class_names):\n",
    "        coords = df[df[\"Label\"] == class_name][[\"Y\", \"X\", \"Z\"]].values\n",
    "        if len(coords) > 0:\n",
    "            coords = coords[0]\n",
    "            landmarks_val[i, j] = coords / spacing_val[i]\n",
    "\n",
    "landmarks_test = np.zeros((len(df_landmarks_test[\"file_id\"].unique()), len(class_names), 3), dtype=np.float32) * np.nan\n",
    "spacing_test = np.zeros((len(df_landmarks_test[\"file_id\"].unique()), 3), dtype=np.float32)\n",
    "for i, file_id in enumerate(df_landmarks_test[\"file_id\"].unique()):\n",
    "    img = nib.load(volume_paths_test[i])\n",
    "    spacing_test[i] = img.header.get_zooms()[:3]\n",
    "    affine = img.affine\n",
    "    affine_inv = np.linalg.inv(affine)\n",
    "    df = df_landmarks_test[df_landmarks_test[\"file_id\"] == file_id]\n",
    "    for j, class_name in enumerate(class_names):\n",
    "        coords = df[df[\"Label\"] == class_name][[\"Y\", \"X\", \"Z\"]].values\n",
    "        if len(coords) > 0:\n",
    "            coords = coords[0]\n",
    "            landmarks_test[i, j] = coords / spacing_test[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_img = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from landmarker.data import MaskDataset\n",
    "\n",
    "ds_train = MaskDataset(\n",
    "    imgs=volume_paths_train,\n",
    "    landmarks=landmarks_train,\n",
    "    spatial_dims=3,\n",
    "    pixel_spacing= spacing_train,\n",
    "    transform=train_transformd,\n",
    "    store_imgs=False,\n",
    "    dim_img=dim_img,\n",
    "    resize_pad=False\n",
    ")\n",
    "\n",
    "ds_val = MaskDataset(\n",
    "    imgs=volume_paths_val,\n",
    "    landmarks=landmarks_val,\n",
    "    spatial_dims=3,\n",
    "    pixel_spacing=pixel_spacing_val,\n",
    "    transform=inference_transformd,\n",
    "    store_imgs=False,\n",
    "    dim_img=dim_img,\n",
    "    resize_pad=False\n",
    ")\n",
    "\n",
    "ds_test = MaskDataset(\n",
    "    imgs=volume_paths_test,\n",
    "    landmarks=landmarks_test,\n",
    "    spatial_dims=3,\n",
    "    pixel_spacing=pixel_spacing_test,\n",
    "    transform=inference_transformd,\n",
    "    store_imgs=False,\n",
    "    dim_img=dim_img,\n",
    "    resize_pad=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = ds_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install 'napari[all]'\n",
    "# %pip install 'ipyvolume'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize 3D Image + Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[174.9549, 233.4400,  96.0000],\n",
       "        [164.6944, 236.5181,  84.0000],\n",
       "        [143.1472, 254.9869,  71.0000],\n",
       "        [115.4438, 264.2214,  58.0000],\n",
       "        [ 98.0010, 259.0912,  44.0000],\n",
       "        [ 99.0272, 219.0752,  28.0000],\n",
       "        [172.9027, 173.9290,  12.0000],\n",
       "        [     nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"landmark\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/vision/lib/python3.10/site-packages/napari/layers/points/points.py:1939: RuntimeWarning: invalid value encountered in cast\n",
      "  ).astype(int)\n",
      "/opt/anaconda3/envs/vision/lib/python3.10/site-packages/napari/layers/points/points.py:1940: RuntimeWarning: divide by zero encountered in divide\n",
      "  zoom_factor = np.divide(\n",
      "/opt/anaconda3/envs/vision/lib/python3.10/site-packages/napari/layers/points/points.py:1956: RuntimeWarning: invalid value encountered in multiply\n",
      "  np.ceil(zoom_factor * np.array(shape[:2])).astype(int),\n",
      "/opt/anaconda3/envs/vision/lib/python3.10/site-packages/napari/layers/points/points.py:1956: RuntimeWarning: invalid value encountered in cast\n",
      "  np.ceil(zoom_factor * np.array(shape[:2])).astype(int),\n",
      "/opt/anaconda3/envs/vision/lib/python3.10/site-packages/napari/layers/points/points.py:1962: RuntimeWarning: invalid value encountered in cast\n",
      "  ).astype(int)\n",
      "/opt/anaconda3/envs/vision/lib/python3.10/site-packages/napari/layers/points/points.py:1939: RuntimeWarning: invalid value encountered in cast\n",
      "  ).astype(int)\n",
      "/opt/anaconda3/envs/vision/lib/python3.10/site-packages/napari/layers/points/points.py:1940: RuntimeWarning: divide by zero encountered in divide\n",
      "  zoom_factor = np.divide(\n",
      "/opt/anaconda3/envs/vision/lib/python3.10/site-packages/napari/layers/points/points.py:1956: RuntimeWarning: invalid value encountered in multiply\n",
      "  np.ceil(zoom_factor * np.array(shape[:2])).astype(int),\n",
      "/opt/anaconda3/envs/vision/lib/python3.10/site-packages/napari/layers/points/points.py:1956: RuntimeWarning: invalid value encountered in cast\n",
      "  np.ceil(zoom_factor * np.array(shape[:2])).astype(int),\n",
      "/opt/anaconda3/envs/vision/lib/python3.10/site-packages/napari/layers/points/points.py:1962: RuntimeWarning: invalid value encountered in cast\n",
      "  ).astype(int)\n"
     ]
    }
   ],
   "source": [
    "import napari\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "viewer = napari.Viewer(ndisplay=3)\n",
    "\n",
    "voxel_spacing = tuple(batch['spacing'].T.tolist())\n",
    "\n",
    "viewer.add_image(batch[\"image\"][0],\n",
    "                 scale=voxel_spacing,\n",
    "                 name=\"image\")\n",
    "\n",
    "# Add landmarks (as points layer)\n",
    "viewer.add_points(\n",
    "    np.stack((batch[\"landmark\"][:, 1], batch[\"landmark\"][:, 0], batch[\"landmark\"][:, 2]), axis=-1),\n",
    "    size=5,\n",
    "    face_color=\"red\",\n",
    "    name=\"landmarks\",\n",
    "    scale=voxel_spacing\n",
    ")\n",
    "\n",
    "napari.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "img = nib.load(ds_train.img_paths[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyvolume as ipv\n",
    "import numpy as np\n",
    "\n",
    "# Show volume\n",
    "ipv.quickvolshow(batch[\"image\"][0], level=[0.3, 0.6], opacity=0.03)\n",
    "\n",
    "# Extract xyz coords\n",
    "z, y, x = batch[\"landmark\"].T\n",
    "\n",
    "# Plot landmarks\n",
    "ipv.scatter(x, y, z, color=\"red\", size=5, marker=\"sphere\")\n",
    "\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize 3D Image + Masks + Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "\n",
    "# Launch viewer\n",
    "viewer = napari.Viewer(ndisplay=3)\n",
    "\n",
    "# Add base volume\n",
    "viewer.add_image(batch[\"image\"][0], name=\"volume\")\n",
    "\n",
    "# Add landmarks (as points layer)\n",
    "viewer.add_points(\n",
    "    batch[\"landmark\"],\n",
    "    size=2,\n",
    "    face_color=\"blue\",\n",
    "    name=\"landmarks\"\n",
    ")\n",
    "\n",
    "# Add each channel as a label layer (colored mask)\n",
    "for c in range(14):\n",
    "    viewer.add_labels(batch[\"mask\"][c].int(), name=f\"landmark_{c}\")\n",
    "\n",
    "napari.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and initializing the Unet model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the model, optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets import FlexibleUNet\n",
    "from landmarker.losses import NLLLoss\n",
    "from landmarker.models.utils import SoftmaxND\n",
    "\n",
    "model = FlexibleUNet(\n",
    "                    in_channels=1,\n",
    "                    out_channels=14, # nb of landmarks\n",
    "                    backbone=\"efficientnet-b0\",\n",
    "                    pretrained=True,\n",
    "                    decoder_channels=[128, 128, 128, 128, 128],\n",
    "                    spatial_dims=3,\n",
    "                    norm=\"batch\",\n",
    "                    act=\"relu\",\n",
    "                    dropout=0.5,\n",
    "                    decoder_bias=False,\n",
    "                    upsample=\"nontrainable\",\n",
    "                    pre_conv=\"default\",\n",
    "                    interp_mode=\"nearest\",\n",
    "                    is_pad=True,\n",
    "                    ).to(device)\n",
    "print(\"Number of learnable parameters: {}\".format(\n",
    "    sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "lr = 1e-5\n",
    "batch_size = 1\n",
    "epochs = 60\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "            [\n",
    "                {\"params\": model.parameters()},\n",
    "            ],\n",
    "            lr=lr,\n",
    ")\n",
    "\n",
    "\n",
    "criterion = NLLLoss(spatial_dims=3)\n",
    "\n",
    "decoder_method = \"argmax\"\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5,\n",
    "                                                          patience=10, cooldown=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(ds_val, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from landmarker.heatmap.decoder import heatmap_to_coord\n",
    "from landmarker.metrics import point_error\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        images = batch[\"image\"].to(device)\n",
    "        masks = batch[\"mask\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 10000.0)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def val_epoch(model, val_loader, criterion, device, method=\"argmax\"):\n",
    "    eval_loss = 0\n",
    "    eval_mpe = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(val_loader)):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            outputs = model(images)\n",
    "            dim_orig = batch[\"dim_original\"].to(device)\n",
    "            pixel_spacing = batch[\"spacing\"].to(device)\n",
    "            padding = batch[\"padding\"].to(device)\n",
    "            masks = batch[\"mask\"].to(device)\n",
    "            landmarks = batch[\"landmark\"].to(device)\n",
    "            loss = criterion(outputs, masks)\n",
    "            pred_landmarks = heatmap_to_coord(outputs, method=method, spatial_dims=3)\n",
    "            eval_loss += loss.item()\n",
    "            eval_mpe += point_error(landmarks, pred_landmarks, images.shape[-3:], dim_orig,\n",
    "                                    pixel_spacing, padding, reduction=\"mean\")\n",
    "    return eval_loss / len(val_loader), eval_mpe / len(val_loader)\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, device, epochs=1000):\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_mpe = val_epoch(model, val_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train loss: {train_loss:.4f} - Val loss: {val_loss:.4f} - Val mpe: {val_mpe:.4f}\")\n",
    "        lr_scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_loader, val_loader, criterion, optimizer, device,\n",
    "      epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"3D-mml-one-hot-unet.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"3D-mml-one-hot-unet.pt\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_landmarks_test = []\n",
    "true_landmarks_test = []\n",
    "dim_origs_test = []\n",
    "pixel_spacings_test = []\n",
    "paddings_test = []\n",
    "test_mpe = 0\n",
    "model.eval()\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(test_loader)):\n",
    "        images = batch[\"image\"].to(device)\n",
    "        outputs = model(images)\n",
    "        dim_orig = batch[\"dim_original\"].to(device)\n",
    "        pixel_spacing = batch[\"spacing\"].to(device)\n",
    "        padding = batch[\"padding\"].to(device)\n",
    "        landmarks = batch[\"landmark\"].to(device)\n",
    "        pred_landmark = heatmap_to_coord(outputs, method=\"argmax\", spatial_dims=3)\n",
    "        test_mpe += point_error(landmarks, pred_landmark, images.shape[-3:], dim_orig,\n",
    "                                pixel_spacing, padding, reduction=\"mean\")\n",
    "        pred_landmarks_test.append(pred_landmark.cpu())\n",
    "        true_landmarks_test.append(landmarks.cpu())\n",
    "        dim_origs_test.append(dim_orig.cpu())\n",
    "        pixel_spacings_test.append(pixel_spacing.cpu())\n",
    "        paddings_test.append(padding.cpu())\n",
    "\n",
    "pred_landmarks_test = torch.cat(pred_landmarks_test)\n",
    "true_landmarks_test = torch.cat(true_landmarks_test)\n",
    "dim_origs_test = torch.cat(dim_origs_test)\n",
    "pixel_spacings_test = torch.cat(pixel_spacings_test)\n",
    "paddings_test = torch.cat(paddings_test)\n",
    "\n",
    "test_mpe /= len(test_loader)\n",
    "\n",
    "print(f\"Test Mean PE: {test_mpe:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from landmarker.metrics import sdr\n",
    "\n",
    "sdr_test = sdr([2.0, 2.5, 3.0, 4.0], true_landmarks=true_landmarks_test, pred_landmarks=pred_landmarks_test,\n",
    "               dim=dim_img, dim_orig=dim_origs_test.int(), pixel_spacing=pixel_spacings_test, padding=paddings_test)\n",
    "\n",
    "print(\"Results on Test Set:\")\n",
    "for key in sdr_test:\n",
    "    print(f\"SDR for {key}mm: {sdr_test[key]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from landmarker.visualize import detection_report\n",
    "\n",
    "print(\"Test Set\")\n",
    "detection_report(true_landmarks_test, pred_landmarks_test, dim=dim_img, dim_orig=dim_origs_test.int(),\n",
    "                    pixel_spacing=pixel_spacings_test, padding=paddings_test, class_names=ds_test.class_names,\n",
    "                    radius=[2.0, 2.5, 3.0, 4.0], digits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from landmarker.visualize import plot_cpe\n",
    "\n",
    "plot_cpe(true_landmarks_test, pred_landmarks_test, dim=dim_img, dim_orig=dim_origs_test.int(),\n",
    "                    pixel_spacing=pixel_spacings_test, padding=paddings_test, class_names=ds_test.class_names,\n",
    "                    group=False, title=\"CPE curve\", save_path=None,\n",
    "                    stat='proportion', unit='mm', kind='ecdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
